{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1536468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PosixPath('/home/bjangley/VPR/vbr/depths/spagna_train0'), PosixPath('/datasets/vbr_slam/spagna/spagna_train0_kitti'), PosixPath('/home/bjangley/VPR/vbr/poses/spagna_train0'), PosixPath('/datasets/vbr_slam/spagna/spagna_train0'))\n"
     ]
    }
   ],
   "source": [
    "### Dataset Index exchange\n",
    "from pathlib import Path \n",
    "scenes = [\"spagna_train0\", \"ciampino_train0\", \"ciampino_train1\", \"campus_train0\", \"campus_train1\"]\n",
    "start_idx = [0,0,30985,0,12048]\n",
    "\n",
    "\n",
    "IMG_ROOT=\"/datasets/vbr_slam\"\n",
    "DEPTH_ROOT=\"/home/bjangley/VPR/vbr/depths\"\n",
    "POSES_ROOT=\"/home/bjangley/VPR/vbr/poses\" \n",
    "\n",
    "def get_paths_from_scene(scene, img_root, depth_root, pose_root):\n",
    "    depth_dir = Path(depth_root)/f\"{scene}\"\n",
    "    scene_name = scene.split(\"_\")[0]\n",
    "    img_dir = Path(img_root)/f\"{scene_name}\"/f\"{scene}_kitti\"\n",
    "    pose_dir = Path(pose_root)/f\"{scene}\"\n",
    "    calib_path = Path (img_root)/f\"{scene_name}\"/f\"{scene}\"\n",
    "    return depth_dir, img_dir, pose_dir, calib_path\n",
    "\n",
    "print(get_paths_from_scene(scenes[0],img_root=IMG_ROOT, depth_root=DEPTH_ROOT, pose_root=POSES_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campus_train0 pairs saved to /home/bjangley/VPR/mast3r-v2/pairs/ciampino_train0_pairs.csv\n",
      "campus_train1 pairs saved to /home/bjangley/VPR/mast3r-v2/pairs/ciampino_train1_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# scene=\"ciampino\"\n",
    "# # Input file and parameters\n",
    "# pairs_file = \"/home/bjangley/VPR/mast3r-v2/pairs/pairs_ciampinov1/ciampino_matches_inliers_fm_top3_anchors_per_query.csv\"\n",
    "# pairs_df = pd.read_csv(pairs_file)\n",
    "\n",
    "# # Define the ranges for campus_train0 and campus_train1\n",
    "# train0_range = [0, 30985]\n",
    "# train1_range = [30985, 49831]\n",
    "\n",
    "\n",
    "# # Create output directories if they don't exist\n",
    "# output_dir = \"/home/bjangley/VPR/mast3r-v2/pairs/\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Output file paths\n",
    "# train0_output = os.path.join(output_dir, f\"{scene}_train0_pairs.csv\")\n",
    "# train1_output = os.path.join(output_dir, f\"{scene}_train1_pairs.csv\")\n",
    "\n",
    "# # Function to filter pairs based on anchor index range and adjust indices\n",
    "# def filter_pairs_by_range(df, anchor_range, index_offset=0):\n",
    "#     filtered_df = df[(df['anchor_idx'] >= anchor_range[0]) & (df['anchor_idx'] < anchor_range[1])].copy()\n",
    "#     # Adjust indices by subtracting the offset\n",
    "#     filtered_df.loc[:, 'anchor_idx'] = filtered_df['anchor_idx'] - index_offset\n",
    "#     filtered_df.loc[:, 'query_idx'] = filtered_df['query_idx'] - index_offset\n",
    "#     return filtered_df\n",
    "\n",
    "# # Filter the DataFrame for campus_train0 and campus_train1\n",
    "# campus_train0_df = filter_pairs_by_range(pairs_df, train0_range)\n",
    "# campus_train1_df = filter_pairs_by_range(pairs_df, train1_range, train1_range[0])\n",
    "\n",
    "# # Save the splits into separate CSV files\n",
    "# campus_train0_df.to_csv(train0_output, index=False)\n",
    "# campus_train1_df.to_csv(train1_output, index=False)\n",
    "\n",
    "# print(f\"campus_train0 pairs saved to {train0_output}\")\n",
    "# print(f\"campus_train1 pairs saved to {train1_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a811bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs before filtering: 945\n",
      "Total pairs after filtering: 832\n",
      "582 124 124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Input file and parameters\n",
    "pairs_file = \"/home/bjangley/VPR/mast3r-v2/pairs/ciampino_train1_pairs.csv\"\n",
    "pairs_df = pd.read_csv(pairs_file)\n",
    "min_inliers = 300\n",
    "\n",
    "# Filter rows based on the minimum number of inliers\n",
    "filtered_rows = pairs_df[pairs_df['num_inliers'] > min_inliers]\n",
    "\n",
    "# Calculate the number of pairs lost due to filtering\n",
    "print(f\"Total pairs before filtering: {len(pairs_df)}\")\n",
    "print(f\"Total pairs after filtering: {len(filtered_rows)}\")\n",
    "\n",
    "# Extract relevant columns\n",
    "anchor_idxs = filtered_rows['anchor_idx'].astype(int).tolist()\n",
    "query_idxs = filtered_rows['query_idx'].astype(int).tolist()\n",
    "\n",
    "# Combine anchor-query pairs into a list\n",
    "# pairs = [(anchor_seqs[idx], anchor_idxs[idx], query_seqs[idx], query_idxs[idx]) for idx in range(len(anchor_idxs))]\n",
    "pairs = [(anchor_idxs[idx], query_idxs[idx]) for idx in range(len(anchor_idxs))]\n",
    "\n",
    "# Shuffle the pairs randomly\n",
    "random.seed(42)  # Set seed for reproducibility\n",
    "random.shuffle(pairs)\n",
    "\n",
    "# Define split proportions\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate split indices\n",
    "total_pairs = len(pairs)\n",
    "train_end = int(total_pairs * train_ratio)\n",
    "val_end = train_end + int(total_pairs * val_ratio)\n",
    "\n",
    "# Split the pairs\n",
    "train_pairs = pairs[:train_end]\n",
    "val_pairs = pairs[train_end:val_end]\n",
    "test_pairs = pairs[val_end:]\n",
    "print(len(train_pairs), len(val_pairs), len(val_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82cc1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file paths\n",
    "scene = \"ciampino_train1\"\n",
    "pairs_path = f\"/home/bjangley/VPR/mast3r-v2/pairs_finetuning/{scene}\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(pairs_path):\n",
    "    os.makedirs(pairs_path)\n",
    "\n",
    "train_output = os.path.join(pairs_path, f\"train_pairs.txt\")\n",
    "val_output = os.path.join(pairs_path, f\"val_pairs.txt\")\n",
    "test_output = os.path.join(pairs_path, f\"test_pairs.txt\")\n",
    "all_output = os.path.join(pairs_path, f\"all_pairs.txt\")\n",
    "# Save the splits into separate files\n",
    "def save_pairs_to_file(pairs, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for pair in pairs:\n",
    "            # Write anchor and query pair in the format: anchor_seq, anchor_idx, query_seq, query_idx\n",
    "            f.write(f\"{pair[0]} {pair[1]}\\n\")\n",
    "\n",
    "save_pairs_to_file(train_pairs, train_output)\n",
    "save_pairs_to_file(val_pairs, val_output)\n",
    "save_pairs_to_file(test_pairs, test_output)\n",
    "save_pairs_to_file(pairs, all_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b6e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(query_idxs))\n",
    "print(len(anchor_idxs))\n",
    "\n",
    "\n",
    "print(\"train\", train_pairs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef709b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scene: ciampino_train1\n",
      "  KITTI path: /datasets/vbr_slam/ciampino/ciampino_train1_kitti\n",
      "  GT path: /datasets/vbr_slam/ciampino/ciampino_train1/ciampino_train1_gt.txt\n",
      "  âœ“ Successfully loaded ciampino_train1\n",
      "Loaded 1 training scenes: ['ciampino_train1']\n"
     ]
    }
   ],
   "source": [
    "from my_vbr_utils.vbr_dataset import VBRDataset, load_scene_calibration\n",
    "from my_vbr_utils.utilities import load_scene_correspondences\n",
    "\n",
    "# --- Config and Dataset Loading ---\n",
    "# Set these variables to configure your scene and utility path\n",
    "scene_name = 'ciampino_train1'\n",
    "vbr_utils_root = '/home/bjangley/VPR/mast3r-v2/my_vbr_utils'\n",
    "\n",
    "config_path = f'{vbr_utils_root}/vbrPaths.yaml'\n",
    "\n",
    "# Load dataset and calibration\n",
    "all_loaded = VBRDataset(config_path, scenes=[scene_name])\n",
    "vbr_scene = all_loaded.get_combined_dataset()\n",
    "calib = load_scene_calibration(\"ciampino\", config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575e8ca",
   "metadata": {},
   "source": [
    "## Save Depth Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752656af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from my_utils.my_vbr_dataset import generate_depth_and_scene_maps\n",
    "import numpy as np\n",
    "OUTPUT_DIR=\"/home/bjangley/VPR/vbr/depths/ciampino_train1\"\n",
    "T_cam_lidar = calib['cam_l'][\"T_cam_lidar\"]\n",
    "K = calib['cam_l']['K']\n",
    "# print(T_cam_lidar, K)\n",
    "for idx in range(len(pairs)):\n",
    "    pair = pairs[idx]\n",
    "    for image in pair:\n",
    "        item = vbr_scene[image]\n",
    "        img_path = item['image']\n",
    "        lidar_pts = item['lidar_points']\n",
    "        if lidar_pts.shape[0] < 5:\n",
    "            print(f\"[{image}] Skipped (no lidar)\")\n",
    "            continue\n",
    "        # Load image to get size\n",
    "        img = Image.open(img_path)\n",
    "        img_shape = img.size[::-1]  # (H, W)\n",
    "        # Generate depth and scene maps\n",
    "        depth, scene = generate_depth_and_scene_maps(lidar_pts, K, T_cam_lidar, img_shape)\n",
    "        # Save .npy\n",
    "        out_path = os.path.join(OUTPUT_DIR, f\"{image:010d}.npy\")\n",
    "        np.save(out_path, depth.astype(np.float32))\n",
    "        # print(f\"[{image}] Saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250f58bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .npy files in /home/bjangley/VPR/vbr/depths/ciampino_train0: 839\n",
      "Number of unique anchors: 479\n",
      "Number of unique queries: 360\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the number of .npy files in the output directory\n",
    "npy_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.npy')]\n",
    "num_npy_files = len(npy_files)\n",
    "\n",
    "# Extract unique anchors and queries from the pairs\n",
    "anchors = set(pair[0] for pair in pairs)\n",
    "queries = set(pair[1] for pair in pairs)\n",
    "\n",
    "num_unique_anchors = len(anchors)\n",
    "num_unique_queries = len(queries)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of .npy files in {OUTPUT_DIR}: {num_npy_files}\")\n",
    "print(f\"Number of unique anchors: {num_unique_anchors}\")\n",
    "print(f\"Number of unique queries: {num_unique_queries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c844bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth poses saved to /home/bjangley/VPR/vbr/poses/ciampino_train0.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from my_utils.transformations import pose_to_se3, se3_to_pose\n",
    "OUTPUT_DIR=\"/home/bjangley/VPR/vbr/poses\"\n",
    "# Output file path\n",
    "poses_output_file = os.path.join(OUTPUT_DIR, \"ciampino_train0.txt\")\n",
    "\n",
    "# Open the file for writing\n",
    "with open(poses_output_file, \"w\") as f:\n",
    "    for idx in range(len(vbr_scene)): \n",
    "        # Get the pose for the current image\n",
    "        pose = vbr_scene.get_pose(idx)\n",
    "        # Transform the pose to the camera frame\n",
    "        # pose_cam = se3_to_pose(T_cam_lidar @ pose_to_se3(pose))\n",
    "        # Write the pose to the file\n",
    "        f.write(f\"{pose[0]:.6f} {pose[1]:.6f} {pose[2]:.6f} {pose[3]:.6f} {pose[4]:.6f} {pose[5]:.6f} {pose[6]:.6f}\\n\")\n",
    "\n",
    "print(f\"Ground truth poses saved to {poses_output_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c8066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pose = vbr_scene.get_pose(1)\n",
    "pose = se3_to_pose(T_cam_lidar @ pose_to_se3(pose))\n",
    "print(T_cam_lidar)\n",
    "# Print the pose in a readable format with decimals\n",
    "print(\"Pose (Readable Format):\")\n",
    "print(f\"x: {pose[0]:.6f}, y: {pose[1]:.6f}, z: {pose[2]:.6f}, \"\n",
    "      f\"qx: {pose[3]:.6f}, qy: {pose[4]:.6f}, qz: {pose[5]:.6f}, qw: {pose[6]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e31d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
