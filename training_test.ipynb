{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e53f9b",
   "metadata": {},
   "source": [
    "### Clear GPU usage -- notebook loads and model and trains, does not clear cached memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f53bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Add this to the top of your notebooks\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check GPU memory usage/\n",
    "\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "        print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8540140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "import os\n",
    "ROOT = \"/home/bjangley/VPR/vbr/spagna_train0_00\"\n",
    "CALIB_YAML = \"/home/bjangley/VPR/vbr/spagna_train0_00/vbr_calib.yaml\"\n",
    "POSES = \"/home/bjangley/VPR/vbr/spagna_train0_gt.txt\"\n",
    "OUTPUT_DIR = os.path.join(ROOT, \"depthmaps_npy\")\n",
    "PAIRS_PATH = \"/home/bjangley/VPR/mast3r-v2/pairsVBR\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65cafcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from my_utils.my_vbr_dataset import vbrDataset, load_calibration, generate_depth_and_scene_maps  # adjust import as needed\n",
    "\n",
    "\n",
    "# # Load dataset and calibration\n",
    "# dataset = vbrDataset(ROOT, POSES)\n",
    "# calib = load_calibration(CALIB_YAML)\n",
    "# K = calib['cam_l']['K']\n",
    "# T_cam_lidar = calib['cam_l']['T_cam_lidar']\n",
    "\n",
    "# # Export first 10 depth maps\n",
    "# for i in range(10):\n",
    "#     item = dataset[i]\n",
    "#     img_path = item['image_left']\n",
    "#     lidar_pts = item['lidar_points']\n",
    "\n",
    "#     if lidar_pts.shape[0] < 5:\n",
    "#         print(f\"[{i}] Skipped (no lidar)\")\n",
    "#         continue\n",
    "\n",
    "#     # Load image to get size\n",
    "#     img = Image.open(img_path)\n",
    "#     img_shape = img.size[::-1]  # (H, W)\n",
    "\n",
    "#     # Generate depth and scene maps\n",
    "#     depth, scene = generate_depth_and_scene_maps(lidar_pts, K, T_cam_lidar, img_shape)\n",
    "\n",
    "#     # Save .npy\n",
    "#     out_path = os.path.join(OUTPUT_DIR, f\"{i:010d}.npy\")\n",
    "#     np.save(out_path, depth.astype(np.float32))\n",
    "#     print(f\"[{i}] Saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd52510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532d72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bjangley/VPR/mast3r-v2/pairsVBR\n",
      "/home/bjangley/VPR/mast3r-v2/pairsVBR/train_pairs.txt\n",
      "Dataset Information:\n",
      "Dataset length: 9\n",
      "Root directory: /home/bjangley/VPR/vbr/spagna_train0_00\n",
      "Dataset loaded using the VBRPairs dataset that inherits from the MASt3RBaseStereoViewDataset \n",
      "\n",
      "img <class 'torch.Tensor'>\n",
      "depthmap <class 'numpy.ndarray'>\n",
      "camera_intrinsics <class 'numpy.ndarray'>\n",
      "camera_pose <class 'numpy.ndarray'>\n",
      "dataset <class 'str'>\n",
      "label <class 'str'>\n",
      "instance <class 'str'>\n",
      "idx <class 'tuple'>\n",
      "is_metric_scale <class 'bool'>\n",
      "pts3d <class 'numpy.ndarray'>\n",
      "valid_mask <class 'numpy.ndarray'>\n",
      "true_shape <class 'numpy.ndarray'>\n",
      "sky_mask <class 'numpy.ndarray'>\n",
      "corres <class 'numpy.ndarray'>\n",
      "valid_corres <class 'numpy.ndarray'>\n",
      "rng <class 'int'>\n",
      "[[  0   0]\n",
      " [ 58   1]\n",
      " [ 24  57]\n",
      " ...\n",
      " [ 54 335]\n",
      " [ 99   3]\n",
      " [ 43 322]]\n",
      "[[ 0.00834208 -0.99996305  0.00208289  0.23782547]\n",
      " [-0.00190743 -0.00209882 -0.999996   -0.66746163]\n",
      " [ 0.9999634   0.0083381  -0.00192488 -0.07445846]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "Dataloader \n",
      "View 0:\n",
      "  img: <class 'torch.Tensor'>\n",
      "  depthmap: <class 'torch.Tensor'>\n",
      "  camera_intrinsics: <class 'torch.Tensor'>\n",
      "  camera_pose: <class 'torch.Tensor'>\n",
      "  dataset: <class 'list'>\n",
      "  label: <class 'list'>\n",
      "  instance: <class 'list'>\n",
      "  idx: <class 'list'>\n",
      "  is_metric_scale: <class 'torch.Tensor'>\n",
      "  pts3d: <class 'torch.Tensor'>\n",
      "  valid_mask: <class 'torch.Tensor'>\n",
      "  true_shape: <class 'torch.Tensor'>\n",
      "  sky_mask: <class 'torch.Tensor'>\n",
      "  corres: <class 'torch.Tensor'>\n",
      "  valid_corres: <class 'torch.Tensor'>\n",
      "  rng: <class 'torch.Tensor'>\n",
      "View 1:\n",
      "  img: <class 'torch.Tensor'>\n",
      "  depthmap: <class 'torch.Tensor'>\n",
      "  camera_intrinsics: <class 'torch.Tensor'>\n",
      "  camera_pose: <class 'torch.Tensor'>\n",
      "  dataset: <class 'list'>\n",
      "  label: <class 'list'>\n",
      "  instance: <class 'list'>\n",
      "  idx: <class 'list'>\n",
      "  is_metric_scale: <class 'torch.Tensor'>\n",
      "  pts3d: <class 'torch.Tensor'>\n",
      "  valid_mask: <class 'torch.Tensor'>\n",
      "  true_shape: <class 'torch.Tensor'>\n",
      "  sky_mask: <class 'torch.Tensor'>\n",
      "  corres: <class 'torch.Tensor'>\n",
      "  valid_corres: <class 'torch.Tensor'>\n",
      "  rng: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from mast3r.datasets.base.vbr_pairs_dataset import VBRPairsDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# Instantiate your dataset\n",
    "dataset = VBRPairsDataset(\n",
    "    root_dir=ROOT,\n",
    "    split=\"train\",\n",
    "    pairs_txt=PAIRS_PATH,\n",
    "    calib_yaml=CALIB_YAML,\n",
    "    poses_txt=POSES,\n",
    "    depth_dir=OUTPUT_DIR,\n",
    "    resolution=[(512, 384)],  # width x height\n",
    "    n_corres=1000,\n",
    "    aug_crop=False\n",
    ")\n",
    "\n",
    "# First, let's examine the basic dataset properties\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "print(f\"Root directory: {dataset.root_dir}\")\n",
    "\n",
    "print(\"Dataset loaded using the VBRPairs dataset that inherits from the MASt3RBaseStereoViewDataset \\n\")\n",
    "# Let's examine a single sample\n",
    "view0, view1 = dataset[0] \n",
    "for key in view0.keys():\n",
    "    print(key, type(view0[key])) \n",
    "print(view1['corres'])\n",
    "print(view1['camera_pose'])\n",
    "print(view1['valid_corres'])\n",
    "# Load a batch from the DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "batch = next(iter(data_loader))\n",
    "print(\"Dataloader \")\n",
    "for i, view in enumerate(batch):\n",
    "    print(f\"View {i}:\")\n",
    "    for key, value in view.items():\n",
    "        print(f\"  {key}: {type(value)}\")\n",
    "        # if isinstance(value, torch.Tensor):\n",
    "        #     print(f\"    Shape: {value.shape}\")\n",
    "        # elif isinstance(value, np.ndarray):\n",
    "        #     print(f\"    Shape: {value.shape}\")\n",
    "        # else:\n",
    "        #     print(f\"    Value: {value}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d837f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor, query = dataset[4]\n",
    "# print(anchor.keys())\n",
    "# print(anchor['img'].shape, anchor['depthmap'].shape)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# img = anchor['img'].permute(1, 2, 0).numpy()  # CxHxW → HxWxC\n",
    "# img_vis = (img + 1.0) / 2.0  # Scale to [0, 1]\n",
    "\n",
    "# plt.imshow(img_vis)\n",
    "# plt.title(\"Anchor Image\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# depth = anchor['depthmap']\n",
    "# valid_mask = depth > 0\n",
    "\n",
    "# # Get coordinates of valid pixels\n",
    "# v_coords, u_coords = np.where(valid_mask)\n",
    "# depth_values = depth[v_coords, u_coords]\n",
    "\n",
    "# # Plot as scatter\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(u_coords, v_coords, c=depth_values, cmap='plasma', s=1, marker='.')\n",
    "# plt.gca().invert_yaxis()  # Match image coordinate system\n",
    "# plt.colorbar(label='Depth (m)')\n",
    "# plt.title(\"Sparse Depth Map (scatter)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# print(query['camera_pose'])  # Should be 4x4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154a805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mast3r.model import AsymmetricMASt3R\n",
    "\n",
    "\n",
    "## this will convert the model to a local version\n",
    "# model = AsymmetricMASt3R.from_pretrained(\"naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric\")\n",
    "# model.save_pretrained(\"/home/bjangley/VPR/mast3r-v2/checkpoints/mast3r_vitlarge_local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c94e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from mast3r.model import AsymmetricMASt3R\n",
    "# from mast3r.fast_nn import fast_reciprocal_NNs\n",
    "# from dust3r.inference import inference\n",
    "\n",
    "# device = 'cuda:6'\n",
    "# checkpoint_path = \"/home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "\n",
    "# # Load model from local checkpoint\n",
    "# model = AsymmetricMASt3R.from_pretrained(checkpoint_path).to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc0d2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor, query = dataset[0]\n",
    "\n",
    "# # --- Prepare views for inference (only 'img' is needed) ---\n",
    "# # Dataset loading\n",
    "# anchor, query = dataset[0]\n",
    "\n",
    "# # Correctly wrap views\n",
    "# view1 = {\n",
    "#     'img': anchor['img'].unsqueeze(0),\n",
    "#     'true_shape': np.int32([anchor['img'].shape[1:]]),\n",
    "#     'idx': 0,\n",
    "#     'instance': '0'\n",
    "# }\n",
    "# view2 = {\n",
    "#     'img': query['img'].unsqueeze(0),\n",
    "#     'true_shape': np.int32([query['img'].shape[1:]]),\n",
    "#     'idx': 1,\n",
    "#     'instance': '1'\n",
    "# }\n",
    "\n",
    "# # Final input format\n",
    "# input_batch = [(view1, view2)]\n",
    "\n",
    "# # Run inference\n",
    "# with torch.no_grad():\n",
    "#     output = inference(input_batch, model, device=device, batch_size=1, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# # --- Extract descriptors ---\n",
    "# desc1 = output['pred1']['desc'].squeeze(0).detach()\n",
    "# desc2 = output['pred2']['desc'].squeeze(0).detach()\n",
    "\n",
    "# # --- Find 2D-2D matches ---\n",
    "# matches_im0, matches_im1 = fast_reciprocal_NNs(desc1, desc2, subsample_or_initxy1=8,\n",
    "#                                                device=device, dist='dot', block_size=8192)\n",
    "\n",
    "# # --- Visualize matches ---\n",
    "# n_viz = 20\n",
    "# num_matches = matches_im0.shape[0]\n",
    "# match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)\n",
    "# viz_matches_im0 = matches_im0[match_idx_to_viz]\n",
    "# viz_matches_im1 = matches_im1[match_idx_to_viz]\n",
    "\n",
    "# image_mean = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)\n",
    "# image_std = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)\n",
    "\n",
    "# viz_imgs = []\n",
    "# for view in [view1, view2]:\n",
    "#     rgb_tensor = view['img'] * image_std + image_mean\n",
    "#     viz_imgs.append(rgb_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "# H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]\n",
    "# img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant')\n",
    "# img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant')\n",
    "# img = np.concatenate((img0, img1), axis=1)\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.imshow(img)\n",
    "# cmap = plt.get_cmap('jet')\n",
    "# for i in range(n_viz):\n",
    "#     (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T\n",
    "#     plt.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)))\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Sample Matches Between Anchor and Query\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89214c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from mast3r.model import AsymmetricMASt3R\n",
    "# from mast3r.fast_nn import fast_reciprocal_NNs\n",
    "# from dust3r.inference import inference\n",
    "\n",
    "\n",
    "# # device = 'cuda:6'\n",
    "# # checkpoint_path = \"/home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "\n",
    "# # model = AsymmetricMASt3R.from_pretrained(checkpoint_path).to(device).eval()\n",
    "\n",
    "# from dust3r.losses import L21\n",
    "# from mast3r.losses import ConfLoss, Regr3D, ConfMatchingLoss, MatchingLoss, InfoNCE\n",
    "\n",
    "# train_criterion = (\n",
    "#     ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\n",
    "#     + 0.075 * ConfMatchingLoss(\n",
    "#         MatchingLoss(InfoNCE(mode='proper', temperature=0.05), negatives_padding=0, blocksize=8192),\n",
    "#         alpha=10.0,\n",
    "#         confmode='mean'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# # Move tensors in the batch to the correct device\n",
    "# def move_to_device(view, device):\n",
    "#     \"\"\"Move all tensors in a view to the specified device.\"\"\"\n",
    "#     for key in view:\n",
    "#         if isinstance(view[key], torch.Tensor):\n",
    "#             view[key] = view[key].to(device)\n",
    "#     return view\n",
    "\n",
    "# # Extract views from the batch\n",
    "# view1 = move_to_device(batch[0], device)\n",
    "# view2 = move_to_device(batch[1], device)\n",
    "\n",
    "# # Run the model forward pass\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(view1, view2)\n",
    "#     print(\"Model forward pass successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "784da683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bjangley/VPR/mast3r-v2/pairsVBR\n",
      "/home/bjangley/VPR/mast3r-v2/pairsVBR/train_pairs.txt\n",
      "... loading model from /home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\n",
      "instantiating : AsymmetricMASt3R(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100',img_size=(512, 512), head_type='catmlp+dpt', output_mode='pts3d+desc24', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), patch_embed_cls='PatchEmbedDust3R', two_confs=True, desc_conf_mode=('exp', 0, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2510951/3337098108.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  loss_scaler = torch.cuda.amp.GradScaler()  # Optional for mixed precision training\n",
      "/tmp/ipykernel_2510951/3337098108.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/bjangley/VPR/mast3r-v2/dust3r/dust3r/inference.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=bool(use_amp)):\n",
      "/home/bjangley/VPR/mast3r-v2/dust3r/dust3r/model.py:205: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/bjangley/VPR/mast3r-v2/dust3r/dust3r/inference.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 33.8849983215332\n",
      "Batch 1, Loss: 31.937904357910156\n",
      "Batch 2, Loss: 34.739349365234375\n",
      "Batch 3, Loss: 34.96986389160156\n",
      "Batch 4, Loss: 34.08148956298828\n",
      "Batch 5, Loss: 34.27620315551758\n",
      "Batch 6, Loss: 6.393279075622559\n",
      "Batch 7, Loss: 5.660723686218262\n",
      "Batch 8, Loss: 6.155587196350098\n",
      "Training for one epoch completed!\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from mast3r.datasets.base.vbr_pairs_dataset import VBRPairsDataset\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from dust3r.losses import L21\n",
    "from mast3r.losses import ConfLoss, Regr3D, ConfMatchingLoss, MatchingLoss, InfoNCE\n",
    "from dust3r.inference import loss_of_one_batch\n",
    "\n",
    "# --- Paths ---\n",
    "ROOT = \"/home/bjangley/VPR/vbr/spagna_train0_00\"\n",
    "CALIB_YAML = \"/home/bjangley/VPR/vbr/spagna_train0_00/vbr_calib.yaml\"\n",
    "POSES = \"/home/bjangley/VPR/vbr/spagna_train0_gt.txt\"\n",
    "PAIRS_PATH=\"/home/bjangley/VPR/mast3r-v2/pairsVBR\"\n",
    "OUTPUT_DIR = os.path.join(ROOT, \"depthmaps_npy\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Dataset ---\n",
    "dataset = VBRPairsDataset(\n",
    "    root_dir=ROOT,\n",
    "    split=\"train\",\n",
    "    pairs_txt=PAIRS_PATH,\n",
    "    calib_yaml=CALIB_YAML,\n",
    "    poses_txt=POSES,\n",
    "    depth_dir=OUTPUT_DIR,\n",
    "    resolution=[(512, 384)],  # width x height\n",
    "    n_corres=1000,\n",
    "    aug_crop=False\n",
    ")\n",
    "\n",
    "# Reduce batch size to avoid CUDA OOM\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# --- Model ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"/home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "model = AsymmetricMASt3R.from_pretrained(checkpoint_path).to(device).train()\n",
    "\n",
    "# --- Loss Function ---\n",
    "train_criterion = ConfLoss(Regr3D(L21, norm_mode='?avg_dis', gt_scale=True), alpha=0.2).to(device)\n",
    "# train_criterion = ConfLoss(Regr3D(L21, norm_mode='?avg_dis'), alpha=0.2) + 0.075*ConfMatchingLoss(MatchingLoss(InfoNCE(mode='proper', temperature=0.05), negatives_padding=0, blocksize=8192), alpha=10.0, confmode='mean')\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "loss_scaler = torch.cuda.amp.GradScaler()  # Optional for mixed precision training\n",
    "\n",
    "# def move_batch_to_device(batch, device):\n",
    "#     \"\"\"Move all tensors in a batch (list of views) to the specified device.\"\"\"\n",
    "#     for view in batch:  # Iterate over each view in the batch\n",
    "#         for key, value in view.items():\n",
    "#             if isinstance(value, torch.Tensor):\n",
    "#                 view[key] = value.to(device)\n",
    "#     return batch\n",
    "# Check batch structure\n",
    "\n",
    "# --- Training for One Epoch ---\n",
    "model.train()\n",
    "# for batch_idx, batch in enumerate(data_loader):\n",
    "#     # Move batch (list of views) to device\n",
    "#     # batch = move_batch_to_device(batch, device)\n",
    "#     # Forward pass\n",
    "#     # Check batch structure\n",
    "\n",
    "#     with torch.cuda.amp.autocast():\n",
    "#         # Use 'train_criterion' instead of undefined 'criterion'\n",
    "#         # print(view0['camera_pose'])\n",
    "#         loss_tuple = loss_of_one_batch(batch, model, train_criterion, device, symmetrize_batch=True, use_amp=True, ret='loss')\n",
    "#         loss, loss_details = loss_tuple\n",
    "#         loss, loss_details = loss_tuple\n",
    "\n",
    "#     # Backward pass -- does not save the model right now\n",
    "#     optimizer.zero_grad()\n",
    "#     loss_scaler.scale(loss).backward()\n",
    "#     loss_scaler.step(optimizer)\n",
    "#     loss_scaler.update()\n",
    "\n",
    "#     # Print loss\n",
    "#     print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "# print(\"Training for one epoch completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb68b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Training for One Epoch ---\n",
    "# model.train()\n",
    "# for batch_idx, batch in enumerate(data_loader):\n",
    "#     # Debug: Check batch structure (batch is a list of views)\n",
    "#     print(f\"Batch type: {type(batch)}\")\n",
    "#     print(f\"Number of views in batch: {len(batch)}\")\n",
    "    \n",
    "#     # Inspect each view in the batch\n",
    "#     for i, view in enumerate(batch):\n",
    "#         print(f\"View {i} keys: {list(view.keys())}\")\n",
    "#         # Check if 'valid_corres' exists\n",
    "#         if 'valid_corres' in view:\n",
    "#             print(f\"View {i} has valid_corres: {view['valid_corres'].shape}\")\n",
    "#         else:\n",
    "#             print(f\"View {i} missing 'valid_corres'\")\n",
    "\n",
    "#     try:\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             loss_tuple = loss_of_one_batch(batch, model, train_criterion, device, \n",
    "#                                          symmetrize_batch=True, use_amp=True, ret='loss')\n",
    "#             loss, loss_details = loss_tuple\n",
    "\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_scaler.scale(loss).backward()\n",
    "#         loss_scaler.step(optimizer)\n",
    "#         loss_scaler.update()\n",
    "\n",
    "#         print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "        \n",
    "#     except KeyError as e:\n",
    "#         print(f\"KeyError in batch {batch_idx}: {e}\")\n",
    "#         print(\"Available keys in views:\")\n",
    "#         for i, view in enumerate(batch):\n",
    "#             print(f\"  View {i}: {list(view.keys())}\")\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in batch {batch_idx}: {e}\")\n",
    "#         break\n",
    "\n",
    "# print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b206c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AsymmetricMASt3R(\n",
      "  (patch_embed): PatchEmbedDust3R(\n",
      "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (mask_generator): RandomMask()\n",
      "  (rope): cuRoPE2D()\n",
      "  (enc_blocks): ModuleList(\n",
      "    (0-23): 24 x Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (rope): cuRoPE2D()\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (enc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_embed): Linear(in_features=1024, out_features=768, bias=True)\n",
      "  (dec_blocks): ModuleList(\n",
      "    (0-11): 12 x DecoderBlock(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (rope): cuRoPE2D()\n",
      "      )\n",
      "      (cross_attn): CrossAttention(\n",
      "        (projq): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (projk): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (projv): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (rope): cuRoPE2D()\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (norm_y): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (dec_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (dec_blocks2): ModuleList(\n",
      "    (0-11): 12 x DecoderBlock(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (rope): cuRoPE2D()\n",
      "      )\n",
      "      (cross_attn): CrossAttention(\n",
      "        (projq): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (projk): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (projv): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (rope): cuRoPE2D()\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (norm_y): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (downstream_head1): Cat_MLP_LocalFeatures_DPT_Pts3d(\n",
      "    (dpt): DPTOutputAdapter_fix(\n",
      "      (scratch): Module(\n",
      "        (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer_rn): ModuleList(\n",
      "          (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (refinenet1): FeatureFusionBlock_custom(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet2): FeatureFusionBlock_custom(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet3): FeatureFusionBlock_custom(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet4): FeatureFusionBlock_custom(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (head): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Interpolate()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (act_postprocess): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (head_local_features): Mlp(\n",
      "      (fc1): Linear(in_features=1792, out_features=7168, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=7168, out_features=6400, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (downstream_head2): Cat_MLP_LocalFeatures_DPT_Pts3d(\n",
      "    (dpt): DPTOutputAdapter_fix(\n",
      "      (scratch): Module(\n",
      "        (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (layer_rn): ModuleList(\n",
      "          (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (refinenet1): FeatureFusionBlock_custom(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet2): FeatureFusionBlock_custom(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet3): FeatureFusionBlock_custom(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (refinenet4): FeatureFusionBlock_custom(\n",
      "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resConfUnit1): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (resConfUnit2): ResidualConvUnit_custom(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (skip_add): FloatFunctional(\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (head): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Interpolate()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (act_postprocess): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (head_local_features): Mlp(\n",
      "      (fc1): Linear(in_features=1792, out_features=7168, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=7168, out_features=6400, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Cat_MLP_LocalFeatures_DPT_Pts3d(\n",
      "  (dpt): DPTOutputAdapter_fix(\n",
      "    (scratch): Module(\n",
      "      (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer_rn): ModuleList(\n",
      "        (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (refinenet1): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet2): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet3): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet4): FeatureFusionBlock_custom(\n",
      "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit_custom(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (head): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Interpolate()\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (act_postprocess): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head_local_features): Mlp(\n",
      "    (fc1): Linear(in_features=1792, out_features=7168, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=7168, out_features=6400, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print(model)\n",
    "print(model)\n",
    "print(model.downstream_head1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04660575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
