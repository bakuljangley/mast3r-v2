{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f53bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Add this to the top of your notebooks\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check GPU memory usage\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "        print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cafcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from my_utils.my_vbr_dataset import vbrDataset, load_calibration, generate_depth_and_scene_maps  # adjust import as needed\n",
    "\n",
    "# Set paths\n",
    "ROOT = \"/home/bjangley/VPR/vbr/spagna_train0_00\"\n",
    "CALIB_YAML = \"/home/bjangley/VPR/vbr/spagna_train0_00/vbr_calib.yaml\"\n",
    "POSES = \"/home/bjangley/VPR/vbr/spagna_train0_gt.txt\"\n",
    "OUTPUT_DIR = os.path.join(ROOT, \"depthmaps_npy\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load dataset and calibration\n",
    "dataset = vbrDataset(ROOT, POSES)\n",
    "calib = load_calibration(CALIB_YAML)\n",
    "K = calib['cam_l']['K']\n",
    "T_cam_lidar = calib['cam_l']['T_cam_lidar']\n",
    "\n",
    "# # Export first 10 depth maps\n",
    "# for i in range(10):\n",
    "#     item = dataset[i]\n",
    "#     img_path = item['image_left']\n",
    "#     lidar_pts = item['lidar_points']\n",
    "\n",
    "#     if lidar_pts.shape[0] < 5:\n",
    "#         print(f\"[{i}] Skipped (no lidar)\")\n",
    "#         continue\n",
    "\n",
    "#     # Load image to get size\n",
    "#     img = Image.open(img_path)\n",
    "#     img_shape = img.size[::-1]  # (H, W)\n",
    "\n",
    "#     # Generate depth and scene maps\n",
    "#     depth, scene = generate_depth_and_scene_maps(lidar_pts, K, T_cam_lidar, img_shape)\n",
    "\n",
    "#     # Save .npy\n",
    "#     out_path = os.path.join(OUTPUT_DIR, f\"{i:010d}.npy\")\n",
    "#     np.save(out_path, depth.astype(np.float32))\n",
    "#     print(f\"[{i}] Saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd52510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532d72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View 0:\n",
      "  img: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 3, 384, 512])\n",
      "  depthmap: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 384, 512])\n",
      "  camera_intrinsics: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 3, 3])\n",
      "  camera_pose: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 4, 4])\n",
      "  dataset: <class 'list'>\n",
      "    Value: ['vbr', 'vbr']\n",
      "  label: <class 'list'>\n",
      "    Value: ['vbr_0000000000', 'vbr_0000000001']\n",
      "  instance: <class 'list'>\n",
      "    Value: ['0', '1']\n",
      "  idx: <class 'list'>\n",
      "    Value: [tensor([0, 1]), tensor([0, 0]), tensor([0, 0])]\n",
      "  is_metric_scale: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2])\n",
      "  pts3d: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 384, 512, 3])\n",
      "  valid_mask: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 384, 512])\n",
      "  true_shape: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 2])\n",
      "  sky_mask: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 384, 512])\n",
      "  corres: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 0, 2])\n",
      "  rng: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2])\n",
      "View 1:\n",
      "  img: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 3, 384, 512])\n",
      "  depthmap: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 384, 512])\n",
      "  camera_intrinsics: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 3, 3])\n",
      "  camera_pose: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 4, 4])\n",
      "  dataset: <class 'list'>\n",
      "    Value: ['vbr', 'vbr']\n",
      "  label: <class 'list'>\n",
      "    Value: ['vbr_0000000001', 'vbr_0000000002']\n",
      "  instance: <class 'list'>\n",
      "    Value: ['1', '2']\n",
      "  idx: <class 'list'>\n",
      "    Value: [tensor([0, 1]), tensor([0, 0]), tensor([1, 1])]\n",
      "  is_metric_scale: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2])\n",
      "  pts3d: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 384, 512, 3])\n",
      "  valid_mask: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 384, 512])\n",
      "  true_shape: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 2])\n",
      "  sky_mask: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 384, 512])\n",
      "  corres: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2, 0, 2])\n",
      "  rng: <class 'torch.Tensor'>\n",
      "    Shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from mast3r.datasets.base.vbr_pairs_dataset import VBRPairsDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# Instantiate your dataset\n",
    "dataset = VBRPairsDataset(\n",
    "    root_dir=ROOT,\n",
    "    pairs_txt='/home/bjangley/VPR/mast3r-v2/pairs_test.txt',\n",
    "    calib_yaml=CALIB_YAML,\n",
    "    poses_txt=POSES,\n",
    "    depth_dir=OUTPUT_DIR,\n",
    "    resolution=[(512, 384)],  # width x height\n",
    "    aug_crop=False\n",
    ")\n",
    "\n",
    "# Load a batch from the DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "batch = next(iter(data_loader))\n",
    "\n",
    "for i, view in enumerate(batch):\n",
    "    print(f\"View {i}:\")\n",
    "    for key, value in view.items():\n",
    "        print(f\"  {key}: {type(value)}\")\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"    Shape: {value.shape}\")\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(f\"    Shape: {value.shape}\")\n",
    "        else:\n",
    "            print(f\"    Value: {value}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d837f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor, query = dataset[4]\n",
    "# print(anchor.keys())\n",
    "# print(anchor['img'].shape, anchor['depthmap'].shape)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# img = anchor['img'].permute(1, 2, 0).numpy()  # CxHxW â†’ HxWxC\n",
    "# img_vis = (img + 1.0) / 2.0  # Scale to [0, 1]\n",
    "\n",
    "# plt.imshow(img_vis)\n",
    "# plt.title(\"Anchor Image\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# depth = anchor['depthmap']\n",
    "# valid_mask = depth > 0\n",
    "\n",
    "# # Get coordinates of valid pixels\n",
    "# v_coords, u_coords = np.where(valid_mask)\n",
    "# depth_values = depth[v_coords, u_coords]\n",
    "\n",
    "# # Plot as scatter\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(u_coords, v_coords, c=depth_values, cmap='plasma', s=1, marker='.')\n",
    "# plt.gca().invert_yaxis()  # Match image coordinate system\n",
    "# plt.colorbar(label='Depth (m)')\n",
    "# plt.title(\"Sparse Depth Map (scatter)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# print(query['camera_pose'])  # Should be 4x4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154a805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mast3r.model import AsymmetricMASt3R\n",
    "\n",
    "\n",
    "## this will convert the model to a local version\n",
    "# model = AsymmetricMASt3R.from_pretrained(\"naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric\")\n",
    "# model.save_pretrained(\"/home/bjangley/VPR/mast3r-v2/checkpoints/mast3r_vitlarge_local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c94e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from /home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\n",
      "instantiating : AsymmetricMASt3R(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100',img_size=(512, 512), head_type='catmlp+dpt', output_mode='pts3d+desc24', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), patch_embed_cls='PatchEmbedDust3R', two_confs=True, desc_conf_mode=('exp', 0, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from mast3r.fast_nn import fast_reciprocal_NNs\n",
    "from dust3r.inference import inference\n",
    "\n",
    "device = 'cuda:6'\n",
    "checkpoint_path = \"/home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "\n",
    "# Load model from local checkpoint\n",
    "model = AsymmetricMASt3R.from_pretrained(checkpoint_path).to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0d2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor, query = dataset[0]\n",
    "\n",
    "# # --- Prepare views for inference (only 'img' is needed) ---\n",
    "# # Dataset loading\n",
    "# anchor, query = dataset[0]\n",
    "\n",
    "# # Correctly wrap views\n",
    "# view1 = {\n",
    "#     'img': anchor['img'].unsqueeze(0),\n",
    "#     'true_shape': np.int32([anchor['img'].shape[1:]]),\n",
    "#     'idx': 0,\n",
    "#     'instance': '0'\n",
    "# }\n",
    "# view2 = {\n",
    "#     'img': query['img'].unsqueeze(0),\n",
    "#     'true_shape': np.int32([query['img'].shape[1:]]),\n",
    "#     'idx': 1,\n",
    "#     'instance': '1'\n",
    "# }\n",
    "\n",
    "# # Final input format\n",
    "# input_batch = [(view1, view2)]\n",
    "\n",
    "# # Run inference\n",
    "# with torch.no_grad():\n",
    "#     output = inference(input_batch, model, device=device, batch_size=1, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# # --- Extract descriptors ---\n",
    "# desc1 = output['pred1']['desc'].squeeze(0).detach()\n",
    "# desc2 = output['pred2']['desc'].squeeze(0).detach()\n",
    "\n",
    "# # --- Find 2D-2D matches ---\n",
    "# matches_im0, matches_im1 = fast_reciprocal_NNs(desc1, desc2, subsample_or_initxy1=8,\n",
    "#                                                device=device, dist='dot', block_size=8192)\n",
    "\n",
    "# # --- Visualize matches ---\n",
    "# n_viz = 20\n",
    "# num_matches = matches_im0.shape[0]\n",
    "# match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)\n",
    "# viz_matches_im0 = matches_im0[match_idx_to_viz]\n",
    "# viz_matches_im1 = matches_im1[match_idx_to_viz]\n",
    "\n",
    "# image_mean = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)\n",
    "# image_std = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)\n",
    "\n",
    "# viz_imgs = []\n",
    "# for view in [view1, view2]:\n",
    "#     rgb_tensor = view['img'] * image_std + image_mean\n",
    "#     viz_imgs.append(rgb_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "# H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]\n",
    "# img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant')\n",
    "# img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant')\n",
    "# img = np.concatenate((img0, img1), axis=1)\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.imshow(img)\n",
    "# cmap = plt.get_cmap('jet')\n",
    "# for i in range(n_viz):\n",
    "#     (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T\n",
    "#     plt.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)))\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Sample Matches Between Anchor and Query\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89214c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from mast3r.fast_nn import fast_reciprocal_NNs\n",
    "from dust3r.inference import inference\n",
    "\n",
    "\n",
    "# device = 'cuda:6'\n",
    "# checkpoint_path = \"/home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "\n",
    "# model = AsymmetricMASt3R.from_pretrained(checkpoint_path).to(device).eval()\n",
    "\n",
    "from dust3r.losses import L21\n",
    "from mast3r.losses import ConfLoss, Regr3D, ConfMatchingLoss, MatchingLoss, InfoNCE\n",
    "\n",
    "train_criterion = (\n",
    "    ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\n",
    "    + 0.075 * ConfMatchingLoss(\n",
    "        MatchingLoss(InfoNCE(mode='proper', temperature=0.05), negatives_padding=0, blocksize=8192),\n",
    "        alpha=10.0,\n",
    "        confmode='mean'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Move tensors in the batch to the correct device\n",
    "def move_to_device(view, device):\n",
    "    \"\"\"Move all tensors in a view to the specified device.\"\"\"\n",
    "    for key in view:\n",
    "        if isinstance(view[key], torch.Tensor):\n",
    "            view[key] = view[key].to(device)\n",
    "    return view\n",
    "\n",
    "# Extract views from the batch\n",
    "view1 = move_to_device(batch[0], device)\n",
    "view2 = move_to_device(batch[1], device)\n",
    "\n",
    "# Run the model forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(view1, view2)\n",
    "    print(\"Model forward pass successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784da683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from /home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\n",
      "instantiating : AsymmetricMASt3R(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100',img_size=(512, 512), head_type='catmlp+dpt', output_mode='pts3d+desc24', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), patch_embed_cls='PatchEmbedDust3R', two_confs=True, desc_conf_mode=('exp', 0, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1453086/4280940145.py:44: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  loss_scaler = torch.cuda.amp.GradScaler()  # Optional for mixed precision training\n",
      "/tmp/ipykernel_1453086/4280940145.py:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 0.049063123762607574\n",
      "Batch 1, Loss: 0.14418229460716248\n",
      "Batch 2, Loss: 0.07510824501514435\n",
      "Batch 3, Loss: 1.137029767036438\n",
      "Batch 4, Loss: 0.9854885339736938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 56\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# def move_batch_to_device(batch, device):\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#     \"\"\"Move all tensors in a batch (list of views) to the specified device.\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#     for view in batch:  # Iterate over each view in the batch\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# --- Training for One Epoch ---\u001b[39;00m\n\u001b[1;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move batch (list of views) to device\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# batch = move_batch_to_device(batch, device)\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Use 'train_criterion' instead of undefined 'criterion'\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_tuple\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_of_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymmetrize_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mast3r/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/mast3r/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mast3r/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/mast3r/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/VPR/mast3r-v2/mast3r/datasets/base/mast3r_base_stereo_view_dataset.py:222\u001b[0m, in \u001b[0;36mMASt3RBaseStereoViewDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    219\u001b[0m     view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpts3d\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pts3d\n\u001b[1;32m    220\u001b[0m     view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m valid_mask \u001b[38;5;241m&\u001b[39m np\u001b[38;5;241m.\u001b[39misfinite(pts3d)\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_crops_from_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviews\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviews\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m                              \u001b[49m\u001b[43maug_crop_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maug_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_crops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_tentative_crops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, view \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(views):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# encode the image\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/VPR/mast3r-v2/mast3r/datasets/base/mast3r_base_stereo_view_dataset.py:94\u001b[0m, in \u001b[0;36mMASt3RBaseStereoViewDataset.generate_crops_from_pair\u001b[0;34m(self, view1, view2, resolution, aug_crop_arg, n_crops, rng)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     93\u001b[0m     view \u001b[38;5;241m=\u001b[39m views[i]\n\u001b[0;32m---> 94\u001b[0m     view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m], view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepthmap\u001b[39m\u001b[38;5;124m'\u001b[39m], view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera_intrinsics\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_crop_resize_if_necessary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                                                                                              \u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdepthmap\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                                                                              \u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcamera_intrinsics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                                                                                              \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                                                                                              \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpts3d\u001b[39m\u001b[38;5;124m'\u001b[39m], view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m depthmap_to_absolute_camera_coordinates(view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepthmap\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    100\u001b[0m                                                                                 view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera_intrinsics\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    101\u001b[0m                                                                                 view[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera_pose\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/VPR/mast3r-v2/mast3r/datasets/base/mast3r_base_stereo_view_dataset.py:77\u001b[0m, in \u001b[0;36mMASt3RBaseStereoViewDataset._crop_resize_if_necessary\u001b[0;34m(self, image, depthmap, intrinsics, resolution, rng, info)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# high-quality Lanczos down-scaling\u001b[39;00m\n\u001b[1;32m     76\u001b[0m target_resolution \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(resolution)\n\u001b[0;32m---> 77\u001b[0m image, depthmap, intrinsics \u001b[38;5;241m=\u001b[39m \u001b[43mcropping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrescale_image_depthmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepthmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintrinsics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_resolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# actual cropping (if necessary) with bilinear interpolation\u001b[39;00m\n\u001b[1;32m     80\u001b[0m offset_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[0;32m~/VPR/mast3r-v2/dust3r/dust3r/datasets/utils/cropping.py:75\u001b[0m, in \u001b[0;36mrescale_image_depthmap\u001b[0;34m(image, depthmap, camera_intrinsics, output_resolution, force)\u001b[0m\n\u001b[1;32m     72\u001b[0m output_resolution \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor(input_resolution \u001b[38;5;241m*\u001b[39m scale_final)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# first rescale the image so that it contains the crop\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_resolution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanczos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscale_final\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbicubic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depthmap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     depthmap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(depthmap, output_resolution, fx\u001b[38;5;241m=\u001b[39mscale_final,\n\u001b[1;32m     78\u001b[0m                           fy\u001b[38;5;241m=\u001b[39mscale_final, interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_NEAREST)\n",
      "File \u001b[0;32m~/VPR/mast3r-v2/dust3r/dust3r/datasets/utils/cropping.py:47\u001b[0m, in \u001b[0;36mImageList.resize\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ImageList(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/VPR/mast3r-v2/dust3r/dust3r/datasets/utils/cropping.py:53\u001b[0m, in \u001b[0;36mImageList._dispatch\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/VPR/mast3r-v2/dust3r/dust3r/datasets/utils/cropping.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages]\n",
      "File \u001b[0;32m~/miniconda3/envs/mast3r/lib/python3.11/site-packages/PIL/Image.py:2328\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2317\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2318\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2319\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2320\u001b[0m         )\n\u001b[1;32m   2321\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2322\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2323\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2324\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2325\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2326\u001b[0m         )\n\u001b[0;32m-> 2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from mast3r.datasets.base.vbr_pairs_dataset import VBRPairsDataset\n",
    "from mast3r.model import AsymmetricMASt3R\n",
    "from dust3r.losses import L21\n",
    "from mast3r.losses import ConfLoss, Regr3D, ConfMatchingLoss, MatchingLoss, InfoNCE\n",
    "from dust3r.inference import loss_of_one_batch\n",
    "\n",
    "# --- Paths ---\n",
    "ROOT = \"/home/bjangley/VPR/vbr/spagna_train0_00\"\n",
    "CALIB_YAML = \"/home/bjangley/VPR/vbr/spagna_train0_00/vbr_calib.yaml\"\n",
    "POSES = \"/home/bjangley/VPR/vbr/spagna_train0_gt.txt\"\n",
    "OUTPUT_DIR = os.path.join(ROOT, \"depthmaps_npy\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Dataset ---\n",
    "dataset = VBRPairsDataset(\n",
    "    root_dir=ROOT,\n",
    "    pairs_txt='/home/bjangley/VPR/mast3r-v2/pairs_test.txt',\n",
    "    calib_yaml=CALIB_YAML,\n",
    "    poses_txt=POSES,\n",
    "    depth_dir=OUTPUT_DIR,\n",
    "    resolution=[(512, 384)],  # width x height\n",
    "    aug_crop=False\n",
    ")\n",
    "\n",
    "# Reduce batch size to avoid CUDA OOM\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# --- Model ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"/home/bjangley/VPR/mast3r-old/checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "model = AsymmetricMASt3R.from_pretrained(checkpoint_path).to(device).train()\n",
    "\n",
    "# --- Loss Function ---\n",
    "train_criterion = ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2).to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "loss_scaler = torch.cuda.amp.GradScaler()  # Optional for mixed precision training\n",
    "\n",
    "# def move_batch_to_device(batch, device):\n",
    "#     \"\"\"Move all tensors in a batch (list of views) to the specified device.\"\"\"\n",
    "#     for view in batch:  # Iterate over each view in the batch\n",
    "#         for key, value in view.items():\n",
    "#             if isinstance(value, torch.Tensor):\n",
    "#                 view[key] = value.to(device)\n",
    "#     return batch\n",
    "\n",
    "# --- Training for One Epoch ---\n",
    "model.train()\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "    # Move batch (list of views) to device\n",
    "    # batch = move_batch_to_device(batch, device)\n",
    "    # Forward pass\n",
    "    with torch.cuda.amp.autocast():\n",
    "        # Use 'train_criterion' instead of undefined 'criterion'\n",
    "        loss_tuple = loss_of_one_batch(batch, model, train_criterion, device, symmetrize_batch=True, use_amp=True, ret='loss')\n",
    "        loss, loss_details = loss_tuple\n",
    "        loss, loss_details = loss_tuple\n",
    "\n",
    "    # Backward pass -- does not save the model right now\n",
    "    optimizer.zero_grad()\n",
    "    loss_scaler.scale(loss).backward()\n",
    "    loss_scaler.step(optimizer)\n",
    "    loss_scaler.update()\n",
    "\n",
    "    # Print loss\n",
    "    print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training for one epoch completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb68b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
